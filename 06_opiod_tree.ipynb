{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook demonstrates the use of Scikit-learn's DecisionTreeClassifier to predict opioid overdoses (OD). It covers the entire process: data quality assessment, feature engineering, model training, evaluation, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CaseStudy_training_data.xlsx'\n",
    "df = pd.read_excel(data, sheet_name='Model_data')\n",
    "df.head()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Quality and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values and duplicates\n",
    "df_cleaned = df.dropna().drop_duplicates()\n",
    "\n",
    "# Drop non-essential columns\n",
    "df_cleaned = df_cleaned.drop(['ID'], axis=1)\n",
    "\n",
    "# Rename columns for better readability\n",
    "df_cleaned = df_cleaned.rename(columns={'rx ds': 'rx_ds'})\n",
    "\n",
    "# Verify the cleaned data\n",
    "print(f\"Cleaned dataset contains {df_cleaned.shape[0]} rows and {df_cleaned.shape[1]} columns.\")\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketize 'rx_ds' into quartiles\n",
    "df_cleaned['rx_ds_bucket'] = pd.qcut(\n",
    "    df_cleaned['rx_ds'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Perform one-hot encoding for 'rx_ds_bucket'\n",
    "df_encoded = pd.get_dummies(df_cleaned['rx_ds_bucket'], prefix='rx_ds_bucket')\n",
    "df_cleaned = pd.concat([df_cleaned, df_encoded],\n",
    "                       axis=1).drop(['rx_ds_bucket'], axis=1)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_cleaned.drop(['OD', 'rx_ds'], axis=1)\n",
    "y = df_cleaned['OD']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Model evaluation - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_hyperparams = {\n",
    "    'criterion': 'entropy',  # Measure for the quality of a split; 'entropy' uses information gain.\n",
    "    'splitter': 'random',  # Strategy for splitting; 'random' chooses a random split, 'best' chooses the optimal one.\n",
    "    'max_depth': 50,  # Maximum depth of the tree; limits how deep the tree can grow to prevent overfitting.\n",
    "    'min_samples_split': 10,  # Minimum number of samples required to split a node; larger values make the tree simpler.\n",
    "    'min_samples_leaf': 4,  # Minimum number of samples required at a leaf node; prevents leaves with too few samples.\n",
    "    'max_features': None,  # Maximum features considered for splitting a node; None means all features are used.\n",
    "    'max_leaf_nodes': 30,  # Maximum number of leaf nodes allowed; limits tree complexity.\n",
    "    'min_impurity_decrease': 0.0,  # Minimum impurity reduction required to split a node; prevents insignificant splits.\n",
    "    'min_weight_fraction_leaf': 0.15,  # Minimum fraction of the sample weights required at a leaf; avoids insignificant leaves.\n",
    "    'class_weight': 'balanced',  # Adjusts class weights inversely proportional to their frequencies to handle imbalance.\n",
    "    'ccp_alpha': 0.0,  # Complexity parameter for pruning; higher values prune more to reduce overfitting.\n",
    "    'random_state': 42  # Seed for randomness to ensure reproducibility of the model results.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model\n",
    "tree_model = DecisionTreeClassifier(**tree_hyperparams)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the metrics for the classifier\n",
    "def calculate_metrics(y_test, y_pred, model, X_test):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "    metrics[\"precision\"] = precision_score(y_test, y_pred)\n",
    "    metrics[\"recall\"] = recall_score(y_test, y_pred)\n",
    "    metrics[\"f1\"] = f1_score(y_test, y_pred)\n",
    "    metrics[\"roc_auc\"] = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    metrics[\"ppv\"] = TP / (TP + FP)\n",
    "    metrics[\"npv\"] = TN / (TN + FN)\n",
    "    metrics[\"specificity\"] = TN / (TN + FP)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate tthe metrics speciffically for the tree model\n",
    "def calculate_tree_metrics(model, X_test, y_test):\n",
    "    tree_metrics = {}\n",
    "    tree_metrics[\"tree_depth\"] = model.get_depth()\n",
    "    tree_metrics[\"num_leaves\"] = model.get_n_leaves()\n",
    "    ccp_path = model.cost_complexity_pruning_path(X_test, y_test)\n",
    "    tree_metrics[\"ccp_alphas\"] = ccp_path.ccp_alphas\n",
    "    tree_metrics[\"impurities\"] = ccp_path.impurities\n",
    "    \n",
    "    return tree_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics\n",
    "y_pred = tree_model.predict(X_test)\n",
    "classification_metrics = calculate_metrics(y_test, y_pred, tree_model, X_test)\n",
    "tree_metrics = calculate_tree_metrics(tree_model, X_test, y_test)\n",
    "\n",
    "# Classification Metrics DataFrame\n",
    "results_classification = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision (PPV)', 'Recall (Sensitivity)', 'Specificity', 'F1 Score', 'Positive Predictive Value (PPV)', 'Negative Predictive Value (NPV)', 'ROC AUC'],\n",
    "    'Value': [classification_metrics['accuracy'], classification_metrics['precision'], classification_metrics['recall'], classification_metrics['specificity'], classification_metrics['f1'], classification_metrics['ppv'], classification_metrics['npv'], classification_metrics['roc_auc']]\n",
    "})\n",
    "\n",
    "# Round numerical values to 2 decimal places\n",
    "results_classification['Value'] = results_classification['Value'].round(2)\n",
    "\n",
    "# Decision Tree Metrics DataFrame\n",
    "results_tree = pd.DataFrame({\n",
    "    'Metric': ['Tree Depth', 'Number of Leaves', 'CCP Alphas', 'Impurities'],\n",
    "    'Value': [tree_metrics['tree_depth'], tree_metrics['num_leaves'], tree_metrics['ccp_alphas'], tree_metrics['impurities']]\n",
    "})\n",
    "\n",
    "# Convert arrays to string for better representation\n",
    "results_tree.loc[results_tree['Metric'].isin(['CCP Alphas', 'Impurities']), 'Value'] = results_tree.loc[results_tree['Metric'].isin(['CCP Alphas', 'Impurities']), 'Value'].apply(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree to nderstand the decision paths and criteria at each node\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_tree(tree_model, filled=True, rounded=True, class_names=[\"Not OD\", \"OD\"], feature_names=X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Feature Importances\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': tree_model.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance for Decision Tree')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "# Probabilities for the positive class\n",
    "y_test_proba = tree_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Complexity vs. Model Performance\n",
    "max_depths = range(1, 20)  # Experiment with different tree depths\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_recalls.append(recall_score(y_train, tree.predict(X_train)))\n",
    "    test_recalls.append(recall_score(y_test, tree.predict(X_test)))\n",
    "\n",
    "# Plot recall vs. tree depth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_recalls, marker='o', label='Train Recall')\n",
    "plt.plot(max_depths, test_recalls, marker='o', label='Test Recall')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Tree Depth vs. Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose:** Visualizes the relationship between tree depth and evaluation metrics (like recall or precision).\n",
    "- **Why it's helpful:** Identifies the optimal tree depth for performance and complexity trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree characteristics and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 11  # Choose a sample index from X_test\n",
    "node_indicator = tree_model.decision_path(X_test).toarray()\n",
    "\n",
    "# Display decision path for the chosen sample\n",
    "print(f\"Decision Path for Sample {sample_index}:\")\n",
    "for i, passed in enumerate(node_indicator[sample_index]):\n",
    "    if passed:\n",
    "        print(f\"Node {i}: {tree_model.tree_.feature[i]} <= {tree_model.tree_.threshold[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the decision paths for specific samples\n",
    "decision_paths = tree_model.decision_path(X_test) # A decision path is the sequence of nodes that a particular sample traverses from the root node to a leaf node in the tree\n",
    "decision_paths_array = decision_paths.toarray()\n",
    "\n",
    "decision_paths_df = pd.DataFrame(decision_paths_array)\n",
    "decision_paths_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning to improve performance, and avoiding overfitting\n",
    "ccp_path = tree_model.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_path.ccp_alphas[:-1], ccp_path.impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Effective Alpha (cost complexity pruning parameter)\")\n",
    "plt.ylabel(\"Total Impurity of Leaves (entropy)\")\n",
    "plt.title(\"Cost Complexity Pruning Path\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each alpha, the tree is pruned by removing nodes that add little to the treeâ€™s predictive power (This indicates that branches are being pruned, and the tree is becoming simpler)\n",
    "- Impurity measures how \"disordered\" or \"impure\" the tree is at a given pruning stage\n",
    "- Impurity increases as pruning removes branches, simplifying the tree\n",
    "- At alpha = 0.0, the tree is unpruned, with the lowest impurity (most complex tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc_opioid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
