{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook demonstrates the use of Scikit-learn's DecisionTreeClassifier to predict opioid overdoses (OD). It covers the entire process: data quality assessment, feature engineering, model training, evaluation, and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score, confusion_matrix\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'CaseStudy_training_data.xlsx'\n",
    "df = pd.read_excel(data, sheet_name='Model_data')\n",
    "df.head()\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Quality and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing values and duplicates\n",
    "df_cleaned = df.dropna().drop_duplicates()\n",
    "\n",
    "# Drop non-essential columns\n",
    "df_cleaned = df_cleaned.drop(['ID'], axis=1)\n",
    "\n",
    "# Rename columns for better readability\n",
    "df_cleaned = df_cleaned.rename(columns={'rx ds': 'rx_ds'})\n",
    "\n",
    "# Verify the cleaned data\n",
    "print(f\"Cleaned dataset contains {df_cleaned.shape[0]} rows and {df_cleaned.shape[1]} columns.\")\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketize 'rx_ds' into quartiles\n",
    "df_cleaned['rx_ds_bucket'] = pd.qcut(\n",
    "    df_cleaned['rx_ds'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "\n",
    "# Perform one-hot encoding for 'rx_ds_bucket'\n",
    "df_encoded = pd.get_dummies(df_cleaned['rx_ds_bucket'], prefix='rx_ds_bucket')\n",
    "df_cleaned = pd.concat([df_cleaned, df_encoded],\n",
    "                       axis=1).drop(['rx_ds_bucket'], axis=1)\n",
    "\n",
    "df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = df_cleaned.drop(['OD', 'rx_ds'], axis=1)\n",
    "y = df_cleaned['OD']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Model evaluation - Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_hyperparams = {\n",
    "    'criterion': 'entropy',  # Measure for the quality of a split; 'entropy' uses information gain.\n",
    "    'splitter': 'random',  # Strategy for splitting; 'random' chooses a random split, 'best' chooses the optimal one.\n",
    "    'max_depth': 50,  # Maximum depth of the tree; limits how deep the tree can grow to prevent overfitting.\n",
    "    'min_samples_split': 10,  # Minimum number of samples required to split a node; larger values make the tree simpler.\n",
    "    'min_samples_leaf': 4,  # Minimum number of samples required at a leaf node; prevents leaves with too few samples.\n",
    "    'max_features': None,  # Maximum features considered for splitting a node; None means all features are used.\n",
    "    'max_leaf_nodes': 30,  # Maximum number of leaf nodes allowed; limits tree complexity.\n",
    "    'min_impurity_decrease': 0.0,  # Minimum impurity reduction required to split a node; prevents insignificant splits.\n",
    "    'min_weight_fraction_leaf': 0.15,  # Minimum fraction of the sample weights required at a leaf; avoids insignificant leaves.\n",
    "    'class_weight': 'balanced',  # Adjusts class weights inversely proportional to their frequencies to handle imbalance.\n",
    "    'ccp_alpha': 0.0,  # Complexity parameter for pruning; higher values prune more to reduce overfitting.\n",
    "    'random_state': 42  # Seed for randomness to ensure reproducibility of the model results.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the model\n",
    "tree_model = DecisionTreeClassifier(**tree_hyperparams)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = tree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate the metrics for the classifier\n",
    "def calculate_metrics(y_test, y_pred, model, X_test):\n",
    "    metrics = {}\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "    metrics[\"precision\"] = precision_score(y_test, y_pred)\n",
    "    metrics[\"recall\"] = recall_score(y_test, y_pred)\n",
    "    metrics[\"f1\"] = f1_score(y_test, y_pred)\n",
    "    metrics[\"roc_auc\"] = roc_auc_score(y_test, model.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    TN, FP, FN, TP = conf_matrix.ravel()\n",
    "    \n",
    "    metrics[\"ppv\"] = TP / (TP + FP)\n",
    "    metrics[\"npv\"] = TN / (TN + FN)\n",
    "    metrics[\"specificity\"] = TN / (TN + FP)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate tthe metrics speciffically for the tree model\n",
    "def calculate_tree_metrics(model, X_test, y_test):\n",
    "    tree_metrics = {}\n",
    "    tree_metrics[\"tree_depth\"] = model.get_depth()\n",
    "    tree_metrics[\"num_leaves\"] = model.get_n_leaves()\n",
    "    ccp_path = model.cost_complexity_pruning_path(X_test, y_test)\n",
    "    tree_metrics[\"ccp_alphas\"] = ccp_path.ccp_alphas\n",
    "    tree_metrics[\"impurities\"] = ccp_path.impurities\n",
    "    \n",
    "    return tree_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the metrics\n",
    "y_pred = tree_model.predict(X_test)\n",
    "classification_metrics = calculate_metrics(y_test, y_pred, tree_model, X_test)\n",
    "tree_metrics = calculate_tree_metrics(tree_model, X_test, y_test)\n",
    "\n",
    "# Classification Metrics DataFrame\n",
    "results_classification = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision (PPV)', 'Recall (Sensitivity)', 'Specificity', 'F1 Score', 'Positive Predictive Value (PPV)', 'Negative Predictive Value (NPV)', 'ROC AUC'],\n",
    "    'Value': [classification_metrics['accuracy'], classification_metrics['precision'], classification_metrics['recall'], classification_metrics['specificity'], classification_metrics['f1'], classification_metrics['ppv'], classification_metrics['npv'], classification_metrics['roc_auc']]\n",
    "})\n",
    "\n",
    "# Round numerical values to 2 decimal places\n",
    "results_classification['Value'] = results_classification['Value'].round(2)\n",
    "\n",
    "# Decision Tree Metrics DataFrame\n",
    "results_tree = pd.DataFrame({\n",
    "    'Metric': ['Tree Depth', 'Number of Leaves', 'CCP Alphas', 'Impurities'],\n",
    "    'Value': [tree_metrics['tree_depth'], tree_metrics['num_leaves'], tree_metrics['ccp_alphas'], tree_metrics['impurities']]\n",
    "})\n",
    "\n",
    "# Convert arrays to string for better representation\n",
    "results_tree.loc[results_tree['Metric'].isin(['CCP Alphas', 'Impurities']), 'Value'] = results_tree.loc[results_tree['Metric'].isin(['CCP Alphas', 'Impurities']), 'Value'].apply(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Decision Tree to nderstand the decision paths and criteria at each node\n",
    "plt.figure(figsize=(20, 20))\n",
    "plot_tree(tree_model, filled=True, rounded=True, class_names=[\"Not OD\", \"OD\"], feature_names=X.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Feature Importances\n",
    "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': tree_model.feature_importances_}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
    "plt.title('Feature Importance for Decision Tree')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "# Probabilities for the positive class\n",
    "y_test_proba = tree_model.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Guess')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree Complexity vs. Model Performance\n",
    "max_depths = range(1, 20)  # Experiment with different tree depths\n",
    "train_recalls = []\n",
    "test_recalls = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    tree.fit(X_train, y_train)\n",
    "    train_recalls.append(recall_score(y_train, tree.predict(X_train)))\n",
    "    test_recalls.append(recall_score(y_test, tree.predict(X_test)))\n",
    "\n",
    "# Plot recall vs. tree depth\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(max_depths, train_recalls, marker='o', label='Train Recall')\n",
    "plt.plot(max_depths, test_recalls, marker='o', label='Test Recall')\n",
    "plt.xlabel('Tree Depth')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Tree Depth vs. Recall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Purpose:** Visualizes the relationship between tree depth and evaluation metrics (like recall or precision).\n",
    "- **Why it's helpful:** Identifies the optimal tree depth for performance and complexity trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree characteristics and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 11  # Choose a sample index from X_test\n",
    "node_indicator = tree_model.decision_path(X_test).toarray()\n",
    "\n",
    "# Display decision path for the chosen sample\n",
    "print(f\"Decision Path for Sample {sample_index}:\")\n",
    "for i, passed in enumerate(node_indicator[sample_index]):\n",
    "    if passed:\n",
    "        print(f\"Node {i}: {tree_model.tree_.feature[i]} <= {tree_model.tree_.threshold[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the decision paths for specific samples\n",
    "decision_paths = tree_model.decision_path(X_test) # A decision path is the sequence of nodes that a particular sample traverses from the root node to a leaf node in the tree\n",
    "decision_paths_array = decision_paths.toarray()\n",
    "\n",
    "decision_paths_df = pd.DataFrame(decision_paths_array)\n",
    "decision_paths_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning to improve performance, and avoiding overfitting\n",
    "ccp_path = tree_model.cost_complexity_pruning_path(X_train, y_train)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ccp_path.ccp_alphas[:-1], ccp_path.impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "plt.xlabel(\"Effective Alpha (cost complexity pruning parameter)\")\n",
    "plt.ylabel(\"Total Impurity of Leaves (entropy)\")\n",
    "plt.title(\"Cost Complexity Pruning Path\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each alpha, the tree is pruned by removing nodes that add little to the tree’s predictive power (This indicates that branches are being pruned, and the tree is becoming simpler)\n",
    "- Impurity measures how \"disordered\" or \"impure\" the tree is at a given pruning stage\n",
    "- Impurity increases as pruning removes branches, simplifying the tree\n",
    "- At alpha = 0.0, the tree is unpruned, with the lowest impurity (most complex tree)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hc_opioid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
